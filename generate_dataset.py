import pandas as pd
import os
import numpy as np
import re 
import unidecode
import random
import preprocess_scrap as augm
from langdetect import detect

import elastic_client as es_client
from preprocess_scrap import augmentation_labels, reduce_labels, clean_vocab_label


def read_scraped(dir_paths,category_name,category_id):
    """
    Creates a csv file for category_id and category_name by uniting all csv found in dir_paths.
    Applys data augmentation if less than 1000 labels in a given category.
    Final csv with 'category_uid' and 'label' columns. Ex: "11_eggs.csv"
    """    
    scrapped_files=[]
    for dir_path in dir_paths:
        for file in os.listdir(dir_path):
            if file.endswith(".csv"):
                print("labels file found {}".format(file))
                scrapped_files.append(os.path.join(dir_path, file))
    
    # creating a DataFrame from all csv 
    df_all_labels = [pd.read_csv(path, header = 0) for path in scrapped_files]    
    df_all_labels = pd.concat([df for df in df_all_labels], sort=True)
    #print(df_all_labels.info())
    df_all_labels.drop(columns=['web-scraper-order','web-scraper-start-url'], inplace=True)
    #df_all_labels.rename({'product':'label'},axis=1, inplace=True)  
    df_all_labels['label']=df_all_labels['label'].apply(lambda x: clean_vocab_label(x))   

    # too many labels, keep random 2000
    if df_all_labels.count()['label'] > 4000:
        print("reducing labels for category {}".format(category_name))
        reduced_labels = reduce_labels(df_all_labels['label'])
        df_all_labels = pd.DataFrame(reduced_labels) 
          
    if df_all_labels.count()['label'] < 1500:
        print("data augmentation for category {}".format(category_name))
        new_labels = augmentation_labels(df_all_labels['label'])
        df_all_labels = pd.DataFrame(new_labels)      
    else:
        # apply random modification on label (50% of crop, lemmatize, shuffle, remove generic)
        df_all_labels['label'] = df_all_labels['label'].apply(lambda x: augm.modify_label(x) if bool(random.getrandbits(1)) else x)  
   
    # get uuid from Elasticsearch
    df_all_labels.insert(0,'category_uid', es_client.get_uuid_by_name(category_name))

    print(df_all_labels.info())  
    df_all_labels.to_csv("../../data/scrapped/products_spain/labels_per_category/{0}_{1}.csv".format(category_id,category_name) , index=False, columns=['category_uid','label'])
    
def generate_category(category_name, category_id, parent_cat):
    """
    Creates a csv file for category_name from different folders organized as follows : sign/parent_cat/category_name
    """
    dir_path_carrefour = "../../data/scrapped/products_spain/carrefour_spain/{0}/{1}".format(parent_cat,category_name)
    dir_path_alcampo = "../../data/scrapped/products_spain/alcampo/{0}/{1}".format(parent_cat,category_name)
    dir_path_eroski = "../../data/scrapped/products_spain/eroski/{0}/{1}".format(parent_cat,category_name)
    dir_paths = [dir_path_carrefour, dir_path_alcampo, dir_path_eroski]
    read_scraped(dir_paths,category_name,category_id)

def generate_dataset():
    """
    Generates training dataset for ML training by aggregating labels per category scrapped from differents sources.
    Creates all_labels.csv
    """
    # get categories names and ids. A file with 'id' and 'category' columns.
    df_category = pd.read_csv("../../data/scrapped/products_spain/category_name_id.csv", delimiter=";", header=0)
   
    for index, row in df_category.iterrows():
        generate_category(row['category_name'], row['id'], row['parent'])
    dir_path_labels = "../../data/scrapped/products_spain/labels_per_category"
    files=[]

    # Get all csv per cartegory generated by generate_category above
    for file in os.listdir(dir_path_labels):
        if file.endswith(".csv"):
            print("labels file found {}".format(file))
            files.append(os.path.join(dir_path_labels, file))
    # creating a DataFrame from all csv 
    df_all_labels = [pd.read_csv(path, header = 0) for path in files]    
    df_all_labels = pd.concat([df for df in df_all_labels], sort=True)
    df_all_labels.dropna(inplace=True, subset=['label'])
    print(df_all_labels.info())
    df_all_labels.to_csv("../../data/scrapped/products_spain/labels_per_category/all_labels.csv" , index=False, columns=['category_uid','label'])

def generate_sign_pgsql():
    files=[]
    df_category = pd.read_csv("../../data/scrapped/products_spain/category_name_id.csv", delimiter=";", header=0)
    for _, row in df_category.iterrows():
        generate_csv_pgsql(row['category_name'], row['id'], row['parent'])
     # Get all csv per cartegory generated by generate_category above
    for file in os.listdir("../../data/scrapped/products_spain/labels_pgsql_carrefour/"):
        if file.endswith(".csv"):
            print("labels file found {}".format(file))
            files.append(os.path.join("../../data/scrapped/products_spain/labels_pgsql_carrefour/", file))
    # creating a DataFrame from all csv 
    df_all_labels = [pd.read_csv(path, header = 0) for path in files]    
    df_all_labels = pd.concat([df for df in df_all_labels], sort=True)
    df_all_labels.dropna(inplace=True, subset=['label'])
    df_all_labels.insert(0,'locale', 'es-ES')
    df_all_labels.to_csv("../../data/scrapped/products_spain/labels_pgsql_carrefour/all_labels.csv" , index=False)

def generate_csv_pgsql(category_name, category_id, parent_cat):    
    uid_sign = '2feee221-9fbc-4cc9-b9dc-f2b7b039d1de'    
    dir_path = "../../data/scrapped/products_spain/carrefour_spain/{0}/{1}".format(parent_cat,category_name)
    scrapped_files=[]
    for file in os.listdir(dir_path):
        if file.endswith(".csv"):
            print("labels file found {}".format(file))
            scrapped_files.append(os.path.join(dir_path, file))
    if scrapped_files:
        df_all_labels = [pd.read_csv(path, header = 0) for path in scrapped_files]    

        df_all_labels = pd.concat([df for df in df_all_labels], sort=True)
        df_all_labels.rename({'web-scraper-start-url':'url'},axis=1, inplace=True)  
        df_all_labels.drop(columns=['web-scraper-order'], inplace=True)
        # get uuid from Elasticsearch
        df_all_labels.insert(0,'ag_category_uid', es_client.get_uuid_by_name(category_name))
        df_all_labels.insert(0,'store_site_uid', uid_sign)
        df_all_labels.to_csv("../../data/scrapped/products_spain/labels_pgsql_carrefour/{0}_{1}.csv".format(category_id,category_name) , index=False)

 

def clean_gramos():
    dir_path = "../../data/scrapped/products_spain/alcampo/creamery/cheese" 
    scrapped_files=[]
    for file in os.listdir(dir_path):
        if file.endswith(".csv"):
            print("labels file found {}".format(file))
            scrapped_files.append(os.path.join(dir_path, file))
    
    for file in scrapped_files:
        print(file)
        df = pd.read_csv(file, header=0)
        print(df.info())
        df.drop_duplicates(inplace = True, subset=['label'])
        print(df.info())
        df = df[~df['label'].str.contains('GramosÂ Aproximados')]
        print(df.info())
        df.to_csv(file, index=False, columns=['web-scraper-order','web-scraper-start-url','label'])
    
        
if __name__ == "__main__":
    es_client.connect_elasticsearch()
    generate_sign_pgsql()
    #generate_dataset()
    #clean_gramos()